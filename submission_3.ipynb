{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pylab import plt, mpl\n",
    "import datetime as dt\n",
    "plt.style.use('seaborn')\n",
    "mpl.rcParams['savefig.dpi'] = 300\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "np.set_printoptions(suppress=True, precision=4)\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from pandas_datareader import DataReader\n",
    "import pandas_datareader as pdr\n",
    "\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keras']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: x.find('keras')+1, list(dir(tf))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractRNNCell',\n",
       " 'Activation',\n",
       " 'ActivityRegularization',\n",
       " 'Add',\n",
       " 'AdditiveAttention',\n",
       " 'AlphaDropout',\n",
       " 'Attention',\n",
       " 'Average',\n",
       " 'AveragePooling1D',\n",
       " 'AveragePooling2D',\n",
       " 'AveragePooling3D',\n",
       " 'AvgPool1D',\n",
       " 'AvgPool2D',\n",
       " 'AvgPool3D',\n",
       " 'BatchNormalization',\n",
       " 'Bidirectional',\n",
       " 'Concatenate',\n",
       " 'Conv1D',\n",
       " 'Conv1DTranspose',\n",
       " 'Conv2D',\n",
       " 'Conv2DTranspose',\n",
       " 'Conv3D',\n",
       " 'Conv3DTranspose',\n",
       " 'ConvLSTM2D',\n",
       " 'Convolution1D',\n",
       " 'Convolution1DTranspose',\n",
       " 'Convolution2D',\n",
       " 'Convolution2DTranspose',\n",
       " 'Convolution3D',\n",
       " 'Convolution3DTranspose',\n",
       " 'Cropping1D',\n",
       " 'Cropping2D',\n",
       " 'Cropping3D',\n",
       " 'Dense',\n",
       " 'DenseFeatures',\n",
       " 'DepthwiseConv2D',\n",
       " 'Dot',\n",
       " 'Dropout',\n",
       " 'ELU',\n",
       " 'Embedding',\n",
       " 'Flatten',\n",
       " 'GRU',\n",
       " 'GRUCell',\n",
       " 'GaussianDropout',\n",
       " 'GaussianNoise',\n",
       " 'GlobalAveragePooling1D',\n",
       " 'GlobalAveragePooling2D',\n",
       " 'GlobalAveragePooling3D',\n",
       " 'GlobalAvgPool1D',\n",
       " 'GlobalAvgPool2D',\n",
       " 'GlobalAvgPool3D',\n",
       " 'GlobalMaxPool1D',\n",
       " 'GlobalMaxPool2D',\n",
       " 'GlobalMaxPool3D',\n",
       " 'GlobalMaxPooling1D',\n",
       " 'GlobalMaxPooling2D',\n",
       " 'GlobalMaxPooling3D',\n",
       " 'Input',\n",
       " 'InputLayer',\n",
       " 'InputSpec',\n",
       " 'LSTM',\n",
       " 'LSTMCell',\n",
       " 'Lambda',\n",
       " 'Layer',\n",
       " 'LayerNormalization',\n",
       " 'LeakyReLU',\n",
       " 'LocallyConnected1D',\n",
       " 'LocallyConnected2D',\n",
       " 'Masking',\n",
       " 'MaxPool1D',\n",
       " 'MaxPool2D',\n",
       " 'MaxPool3D',\n",
       " 'MaxPooling1D',\n",
       " 'MaxPooling2D',\n",
       " 'MaxPooling3D',\n",
       " 'Maximum',\n",
       " 'Minimum',\n",
       " 'Multiply',\n",
       " 'PReLU',\n",
       " 'Permute',\n",
       " 'RNN',\n",
       " 'ReLU',\n",
       " 'RepeatVector',\n",
       " 'Reshape',\n",
       " 'SeparableConv1D',\n",
       " 'SeparableConv2D',\n",
       " 'SeparableConvolution1D',\n",
       " 'SeparableConvolution2D',\n",
       " 'SimpleRNN',\n",
       " 'SimpleRNNCell',\n",
       " 'Softmax',\n",
       " 'SpatialDropout1D',\n",
       " 'SpatialDropout2D',\n",
       " 'SpatialDropout3D',\n",
       " 'StackedRNNCells',\n",
       " 'Subtract',\n",
       " 'ThresholdedReLU',\n",
       " 'TimeDistributed',\n",
       " 'UpSampling1D',\n",
       " 'UpSampling2D',\n",
       " 'UpSampling3D',\n",
       " 'Wrapper',\n",
       " 'ZeroPadding1D',\n",
       " 'ZeroPadding2D',\n",
       " 'ZeroPadding3D',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " '_sys',\n",
       " 'add',\n",
       " 'average',\n",
       " 'concatenate',\n",
       " 'deserialize',\n",
       " 'dot',\n",
       " 'experimental',\n",
       " 'maximum',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'serialize',\n",
       " 'subtract']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tf.keras.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'svg'\n",
    "#To ignore all warnings in model output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "random.seed(50) #for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#baba = DataReader('BABA',  'yahoo', datetime(2014,9,14), datetime(2020,10,16))\n",
    "baba= pdr.data.get_data_yahoo('BABA',datetime(2014,9,14), datetime(2020,10,16))\n",
    "baba = pd.DataFrame(baba.loc[:,'Adj Close'])\n",
    "baba.columns = ['BABA']\n",
    "data = baba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous section lays out the blueprint for vectorized backtesting on the basis of a simple, easy-to-visualize trading strategy. We are going to apply vectorized backtesting to DNN-based trading strategies.The following trains a Keras DNN model. We are going to use the following technical indicators as features in our model.\n",
    "returns, minimun price over a window, maximum price over a window, the momentum indicator and the voloatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "non-default argument follows default argument (<ipython-input-24-b567744e3fc2>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-24-b567744e3fc2>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    def add_lags(dataframe, symbol, lags=5, features, window=20):\u001b[0m\n\u001b[1;37m                ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m non-default argument follows default argument\n"
     ]
    }
   ],
   "source": [
    "def add_lags(dataframe, symbol, lags=5, features, window=20):\n",
    "    cols = []\n",
    "    df = dataframe.copy()\n",
    "    df.dropna(inplace=True)\n",
    "    df['r'] = np.log(df / df.shift(1))\n",
    "    df['sma'] = df[symbol].rolling(window).mean()\n",
    "    df['min'] = df[symbol].rolling(window).min()\n",
    "    df['max'] = df[symbol].rolling(window).max()\n",
    "    df['mom'] = df['r'].rolling(window).mean()\n",
    "    df['vol'] = df['r'].rolling(window).std()\n",
    "    df.dropna(inplace=True)\n",
    "    df['d'] = np.where(df['r'] > 0, 1, 0)\n",
    "    for f in features:\n",
    "        for lag in range(1, lags + 1):\n",
    "            col = f'{f}_lag_{lag}'\n",
    "            df[col] = df[f].shift(lag)\n",
    "            cols.append(col)\n",
    "    df.dropna(inplace=True)\n",
    "    return df, cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose `lags=5` and `features = [symbol, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']` to generate the data for our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lags = 5\n",
    "features = [symbol, 'r', 'd', 'sma', 'min', 'max', 'mom', 'vol']\n",
    "data, cols = add_lags(data, symbol, lags,features, window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['BABA', 'r', 'sma', 'min', 'max', 'mom', 'vol']\n",
    "data, cols = add_lags(data, 'BABA', lags,features, window=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hl=2, hu=128, dropout=False, rate=0.3,\n",
    "                 regularize=False, reg=l1(0.0005),\n",
    "                 optimizer=optimizer, input_dim=len(cols)):\n",
    "    model = Sequential()\n",
    "    if not regularize:\n",
    "        reg = None\n",
    "    model.add(Dense(hu, input_dim=input_dim,\n",
    "                          activity_regularizer=reg,\n",
    "                          activation='relu'))\n",
    "    if dropout:\n",
    "        model.add(Dropout(rate, seed=100))\n",
    "    for _ in range(hl):\n",
    "        model.add(Dense(hu, activation='relu',\n",
    "                        activity_regularizer=reg))\n",
    "        if dropout:\n",
    "            model.add(Dropout(rate, seed=100))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer=optimizer,\n",
    "                   metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train-test split the historical data on a sequential manner, and train the DNN model based on normalized features data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit #better way of splitting to be used later after successfully running first version of model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of training and testing samples and model creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use 10% of the samples for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_numbers = data.shape[0]\n",
    "test_sample = int(0.1 * sample_numbers)\n",
    "#delta =dt.timedelta(-test_sample)\n",
    "#split_point = data.index[-1] + delta\n",
    "split_point = list(data.index)[sample_numbers - test_sample]\n",
    "train = data.loc[:split_point].copy()\n",
    "test = data.loc[split_point:]\n",
    "#scaler = StandardScaler()\n",
    "mu, std = train.mean(), train.std()\n",
    "train_ = (train - mu) / std\n",
    "test = data.loc[split_point:].copy()\n",
    "test_ = (test - mu) / std\n",
    "#train_ = scaler.fit_transform(train)\n",
    "model = create_model(hl=2, hu=64, dropout=True, regularize=True)\n",
    "model.fit(train_[cols], train['d'],\n",
    "                 epochs=20, verbose=False,\n",
    "                 validation_split=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(train_[cols], train['d']) \n",
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorized backtesting can now be applied to judge the economic performance of the DNN-based trading strategy in-sample based on the modelâ€™s predictions.\n",
    "In this context, an upward prediction is naturally interpreted as a long position and a downward prediction as a short position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['p'] = np.where(model.predict(train_[cols]) > 0.5, 1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,['p','r']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['s'] = train['p'] * train['r']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.loc[:,['p','r','s']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_returns = train[['r', 's']].add(1).cumprod().sub(1)\n",
    "cumulative_returns.plot(figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_[cols], test['d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['p'] = np.where(model.predict(test_[cols]) > 0.5, 1, -1)\n",
    "test['p'].value_counts()\n",
    "test['s'] = test['p'] * test['r']\n",
    "cumulative_returns_test = test[['r', 's']].add(1).cumprod().sub(1)\n",
    "cumulative_returns_test.plot(figsize=(10, 6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['p'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas to improve the model\n",
    "- Add addtitional indicators such bollingers band, etc (checkout the packages for algorithmic trading)\n",
    "- suggest adding sentiment analysis, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model evaluation\n",
    "Calculate Sharpe Ratio, Maxdrawdown, ... etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add references\n",
    "- Machine learning for AI, Yves Ipsich\n",
    "- [python for trading](https://www.datacamp.com/community/tutorials/finance-python-trading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
